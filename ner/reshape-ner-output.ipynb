{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts to reshape output from named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove bracketed words from plain text ground truth transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'input-transcripts/aws-transcribe/astin-patten_aws_transcript.txt'\n",
    "filestem = filename.split('/')[-1][0:-4]\n",
    "with open(filename, 'r') as f:\n",
    "    text = f.read()\n",
    "    output = re.sub('\\[[a-zA-Z,\\s]*\\]', '', text)\n",
    "zname = 'Transcripts/' + filestem + '_for_NLP.txt'\n",
    "z = open(zname, 'w')\n",
    "z.write(output)\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert IBM Watson json to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ents(data):\n",
    "    entities = []\n",
    "    ents = data['entities']\n",
    "    for e in ents:\n",
    "        entity = {}\n",
    "        entity['ibm_type'] = 'entity'\n",
    "        entity['type'] = e['type']\n",
    "        entity['text'] = e['text']\n",
    "        entity['relevance'] = e['relevance']\n",
    "        if 'disambiguation' in e:\n",
    "            entity['subtypes'] = ','.join(e['disambiguation']['subtype'])\n",
    "        entities.append(entity)\n",
    "    return entities\n",
    "\n",
    "def parse_concepts(data):\n",
    "    concepts = []\n",
    "    cons = data['concepts']\n",
    "    for c in cons:\n",
    "        concept = {}\n",
    "        concept['ibm_type'] = 'concept'\n",
    "        concept['text'] = c['text']\n",
    "        concept['relevance'] = c['relevance']\n",
    "        concepts.append(concept)\n",
    "    return concepts   \n",
    "\n",
    "def parse_keywords(data):\n",
    "    keywords = []\n",
    "    keys = data['keywords']\n",
    "    for k in keys:\n",
    "        keyword = {}\n",
    "        keyword['ibm_type'] = 'keyword'\n",
    "        keyword['text'] = k['text']\n",
    "        keyword['relevance'] = k['relevance']\n",
    "        keywords.append(keyword)\n",
    "    return keywords\n",
    "\n",
    "def write_to_csv(filename, entities):\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        fieldnames = ['ibm_type', 'type', 'text', 'relevance', 'subtypes']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for e in entities:\n",
    "            writer.writerow(e)\n",
    "\n",
    "def get_all_terms(data):\n",
    "    entities = parse_ents(data)\n",
    "    concepts = parse_concepts(data)\n",
    "    keywords = parse_keywords(data)\n",
    "    all = entities + concepts + keywords\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit the filepath for the specific transcript input file you want to convert\n",
    "women_and_aids = 'ibm-watson/women-and-aids/kaldi-input/women-and-aids_kaldi-input_ner_ibm.json'\n",
    "student_admin = 'ibm-watsom/student-admin-forum/kaldi-input/student-admin-forum_kaldi-input_ner_igm.json'\n",
    "astin_patten = 'ibm-watson/astin-patten/kaldi-input/astin-patten_kaldi-input_ner_ibm.json'\n",
    "\n",
    "jsonfiles = [women_and_aids, student_admin, astin_patten]\n",
    "for j in jsonfiles:\n",
    "    filestem = j[0:-5]\n",
    "    data = json.load(open(j, 'r'))\n",
    "    ents = get_all_terms(data)\n",
    "    write_to_csv(filestem + '.csv', ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Google Natural Language API JSON to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ents(data):\n",
    "    entities = []\n",
    "    ents = data['entities']\n",
    "    for e in ents:\n",
    "        entity = {}\n",
    "        entity['type'] = e['type']\n",
    "        entity['text'] = e['name']\n",
    "        entity['salience'] = e['salience']\n",
    "        entity['num_mentions'] = len(e['mentions'])\n",
    "        # if \"PROPER\" is type for at least one mention, call the entity a proper noun, else common.\n",
    "        mentiontypes = [m['type'] for m in e['mentions']]\n",
    "        if 'PROPER' in mentiontypes:\n",
    "            entity['proper_or_common'] = 'PROPER'\n",
    "        else:\n",
    "            entity['proper_or_common'] = 'COMMON'\n",
    "        entity['mentions'] = ', '.join([m['text']['content'] for m in e['mentions']])\n",
    "        entities.append(entity)\n",
    "    return entities\n",
    "\n",
    "def write_to_csv(filename, entities):\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        fieldnames = ['type', 'text', 'proper_or_common', 'salience', 'num_mentions', 'mentions']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for e in entities:\n",
    "            writer.writerow(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit the filepath for the specific transcript input file you want to convert\n",
    "women_and_aids = 'google-nlp/women-and-aids/ground-truth-input/women-and-aids_gt-input_ner_google.json'\n",
    "student_admin = 'google-nlp/student-admin/ground-truth-input/student-admin-forum_gt-input_ner_google.json'\n",
    "astin_patten = 'google-nlp/astin-patten/ground-truth-input/astin-patten_gt-input_ner_google.json'\n",
    "jsonfiles = [women_and_aids, student_admin, astin_patten]\n",
    "for j in jsonfiles:\n",
    "    filestem = j[0:-5]\n",
    "    data = json.load(open(j, 'r'))\n",
    "    ents = parse_ents(data)\n",
    "    write_to_csv(filestem + '.csv', ents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Comprehend to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ents(data):\n",
    "    entities = []\n",
    "    ents = data['Entities']\n",
    "    for e in ents:\n",
    "        entity = {}\n",
    "        entity['type'] = e['Type']\n",
    "        entity['text'] = e['Text']\n",
    "        entity['score'] = e['Score']\n",
    "        entity['start'] = e['BeginOffset']\n",
    "        entity['end'] = e['EndOffset']\n",
    "        entities.append(entity)\n",
    "    return entities\n",
    "\n",
    "def write_to_csv(filename, entities):\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        fieldnames = ['type', 'text', 'score', 'start', 'end']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for e in entities:\n",
    "            writer.writerow(e)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit the filepath for the specific transcript input file you want to convert\n",
    "women_and_aids = 'aws-comprehend/women-and-aids/ground-truth-input/women-and-aids_gt-input_ner_aws.json'\n",
    "student_admin = 'aws-comprehend/student-admin-forum/ground-truth-input/student-admin-forum_gt-input_ner_aws.json'\n",
    "astin_patten = 'aws-comprehend/astin-patten/ground-truth-input/astin-patten_gt-input_ner_aws.json'\n",
    "jsonfiles = [women_and_aids, student_admin, astin_patten]\n",
    "for j in jsonfiles:\n",
    "    filestem = j[0:-5]\n",
    "    data = json.load(open(j, 'r'))\n",
    "    ents = parse_ents(data)\n",
    "    write_to_csv(filestem + '.csv', ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in jsonfiles:\n",
    "    filestem = j[0:-5]\n",
    "    data = json.load(open(j, 'r'))\n",
    "    ents = parse_ents(data)\n",
    "    write_to_csv(filestem + '.csv', ents)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ents(data):\n",
    "    \"\"\"Extract tags (in format \"word/tag\") with types and join continguous tags with matching types (ex. United/LOCATION\n",
    "     States/LOCATION = United States)\"\"\"\n",
    "    entities = []\n",
    "    fulltext = [line.replace('\\n', '') for line in data]\n",
    "    fulltext = ''.join(fulltext).replace('  ', ' ')\n",
    "    words = fulltext.split(' ')\n",
    "    lasttag = ['','O']\n",
    "    for w in words:\n",
    "        if w not in ['\\n']:\n",
    "            tag = w.split('/')\n",
    "            if len(tag) > 1:\n",
    "                if lasttag[1] != 'O':\n",
    "                    if entities[-1]['type'] == tag[1]:\n",
    "                        print(entities[-1])\n",
    "                        print(tag)\n",
    "                        entities[-1]['text'] = entities[-1]['text'] + ' ' + tag[0]             \n",
    "                elif tag[1] != 'O':\n",
    "                    entity = {}\n",
    "                    entity['text'] = tag[0]\n",
    "                    entity['type'] = tag[1]\n",
    "                    entities.append(entity)\n",
    "                lasttag = tag\n",
    "    return entities\n",
    "\n",
    "def write_to_csv(filename, entities):\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        fieldnames = ['type', 'text']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for e in entities:\n",
    "            writer.writerow(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "women_and_aids = 'stanford/women-and-aids/ground-truth-input/women-and-aids_gt-input_ner_stanford.txt'\n",
    "student_admin = 'stanford/student-admin-forum/ground-truth-input/student-admin-forum_gt-input_ner_stanford.txt'\n",
    "astin_patten = 'stanford/astin-patten/ground-truth-input/women-and-aids_gt-input_ner_stanford.txt'\n",
    "txtfiles = [women_and_aids, student_admin, astin_patten]\n",
    "for t in txtfiles:\n",
    "    filestem = t[0:-4]\n",
    "    data = open(t, 'r')\n",
    "    ents = parse_ents(data)\n",
    "    write_to_csv(filestem + '.csv', ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
